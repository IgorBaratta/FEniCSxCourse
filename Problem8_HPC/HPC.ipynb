{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgorBaratta/FEniCSxCourse/blob/main/Problem8_HPC/HPC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "no6dI0BMtdaN"
      },
      "source": [
        "# DOLFINx in Parallel with MPI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX9ypdWitdaQ"
      },
      "source": [
        "To run the programs in this section, first you will need to install the dolfinx and the mpi4py library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yINZsUrStdaR"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import dolfinx\n",
        "except ImportError:\n",
        "  !wget \"https://github.com/fem-on-colab/fem-on-colab.github.io/raw/7f220b6/releases/fenicsx-install-real.sh\" -O \"/tmp/fenicsx-install.sh\" && bash \"/tmp/fenicsx-install.sh\"\n",
        "  import dolfinx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2st97E6stdaU"
      },
      "source": [
        "## First parallel program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_iWLY3jtdaU"
      },
      "outputs": [],
      "source": [
        "%%writefile hello-world.py\n",
        "from mpi4py import MPI\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "print(f\"Hello world from rank {comm.rank} of {comm.size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIpDhcgftdaV"
      },
      "source": [
        "Next we see how we can use the mpirun program to execute the above python code using 4 processes. The value after -np is the number of processes to use when running the file of python code saved when executing the previous code cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGRDhL5VtdaW"
      },
      "outputs": [],
      "source": [
        "! mpirun --allow-run-as-root --oversubscribe -np 8 python hello-world.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGpuCWwqtdaW"
      },
      "source": [
        "## Point-to-Point Communication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9tL-xSRtdaX"
      },
      "outputs": [],
      "source": [
        "%%writefile p2p.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import numpy\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "assert comm.size == 2\n",
        "rank = comm.rank\n",
        "\n",
        "if rank == 0:\n",
        "    data = numpy.arange(10, dtype=int)\n",
        "    comm.Send(data, dest=1)\n",
        "    print(f\"Process 0 sent {data} to process 1\")\n",
        "elif rank == 1:\n",
        "    data = numpy.zeros(10, dtype=int)\n",
        "    comm.Recv(data, source=0)\n",
        "    print(f\"Process 1 received {data} from process 0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJGoMbK-tdaY"
      },
      "outputs": [],
      "source": [
        "!mpirun -n 2 --allow-run-as-root --oversubscribe python3 p2p.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AX7M36ctdaZ"
      },
      "source": [
        "MPI can do a lot more than this (https://mpi4py.readthedocs.io/en/stable/tutorial.html). The key points are:\n",
        " - N identical versions of your program run, one on each process (rank). Each process takes different paths through the program depending on its rank.\n",
        " - Processes (and hence their memory) are totally independent.\n",
        " - Communication between processes is must be manually performed by the programmer (explicit)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a mesh in parallel\n",
        "DOLFINx abstracts most of the difficult aspects of distributing your problem across the MPI communicator away from the user. \n",
        "Let's have a look at how to create and write a mesh in parallel: "
      ],
      "metadata": {
        "id": "DUDrPpVAvIew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mesh.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "DG = dolfinx.fem.FunctionSpace(mesh, (\"DG\", 0))\n",
        "cell_value = dolfinx.fem.Function(DG)\n",
        "cell_value.x.array[:] = comm.rank\n",
        "\n",
        "# Save mesh and \n",
        "with XDMFFile(comm, f\"mesh_{comm.size}.xdmf\", \"w\") as xdmf:\n",
        "    xdmf.write_mesh(mesh)\n",
        "    xdmf.write_function(cell_value)"
      ],
      "metadata": {
        "id": "inBTmqnQwCxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 mesh.py"
      ],
      "metadata": {
        "id": "nRIAhTfZxI9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try changing the number of processes and visualizing the mesh."
      ],
      "metadata": {
        "id": "1_jGbGP70ogM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Managing data in parallel: the IndexMap\n",
        "\n",
        "The IndexMap represents the distribution index arrays across processes. An index array is a contiguous collection of N+1 indices `[0, 1, . . ., N]` that are distributed across M processes. On a given process, the IndexMap stores a portion of the index set using local indices `[0, 1, . . . , n]`, and a map from the local indices to a unique global index.\n",
        "\n",
        "Let's have a look at an example. \n",
        "How many cells are owned by a given process?"
      ],
      "metadata": {
        "id": "r2IAK2TC1Jpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile index_map.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "tdim = mesh.topology.dim\n",
        "idx_map = mesh.topology.index_map(tdim)\n",
        "\n",
        "N = idx_map.size_global\n",
        "n = idx_map.size_local\n",
        "print(f\"Process {comm.rank} owns {n} cells of {N}\")\n",
        "\n",
        "comm.Barrier()\n",
        "print(\"\")\n",
        "comm.Barrier()\n",
        "\n",
        "vmap = mesh.topology.index_map(0)\n",
        "print(f\"Process {comm.rank} owns {vmap.size_local} vertices of {vmap.size_global}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "a5WPwb2w1-RH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 index_map.py"
      ],
      "metadata": {
        "id": "zhEwpNJ73BXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function and FunctionSpaces\n",
        "\n",
        "The distribution of a vector of function across the MPI process is also managed by an IndexMap and it follows the distribution of the mesh. \n",
        "\n",
        "Consider a continuous first-order Lagrange space over a mesh. The degrees of freedom of this space are associated with the vertices of the mesh.\n",
        "\n",
        "A function in DOLFINx contains memory (an array) in which the expansion coefficients ($u_i$) of the finite element basis ($\\phi_i$) can be stored:\n",
        "\n",
        "$$\n",
        "u_h = \\sum_{i = 0}^{N-1} \\phi_i u_i\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "36puB8S_3XLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile function.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 20, 20)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "\n",
        "map = V.dofmap.index_map\n",
        "\n",
        "print(f\"Process {comm.rank} owns {map.size_local} dofs of {map.size_global}\")\n",
        "\n",
        "comm.Barrier()\n",
        "print(\"\")\n",
        "comm.Barrier()\n",
        "\n",
        "print(f\"Local size {u.x.array.size} in process {comm.rank}\")\n",
        "\n",
        "# The size of local vector includes owned dofs and ghosts\n",
        "assert u.x.array.size ==  map.size_local + map.num_ghosts"
      ],
      "metadata": {
        "id": "JWOg3hGZ4FRP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 4 --allow-run-as-root --oversubscribe python3 function.py"
      ],
      "metadata": {
        "id": "Xmk0AanD6RXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scattering values\n",
        "\n",
        "Letâ€™s say we want to change the expansion coefficient $u_i$ (local numbering) each processes to have a value equal to the MPI rank + 1 of the owning process. For consistency we need this change to be reflected in two places:\n",
        "\n",
        " - In the memory of the process that owns the degree of freedom.\n",
        " - In the memory of the process (if any) that has the degree of freedom as a ghost.\n",
        "\n",
        "\n",
        "How do we do that?"
      ],
      "metadata": {
        "id": "8hK_VdvV7s-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scatter.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "import dolfinx\n",
        "from dolfinx.io import XDMFFile\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 5, 5)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "\n",
        "u.x.array[:] = comm.rank\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {u.x.array}\")\n",
        "    comm.Barrier()\n",
        "\n",
        "comm.Barrier()\n",
        "print(\"\")\n",
        "comm.Barrier()\n",
        "\n",
        "# Scatter values to sharing process\n",
        "u.x.scatter_forward()\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {u.x.array}\")\n",
        "    comm.Barrier()"
      ],
      "metadata": {
        "id": "_E-23bXY8FJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 4 --allow-run-as-root --oversubscribe python3 scatter.py"
      ],
      "metadata": {
        "id": "tfqd0WdR8aic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling vectors in parallel\n",
        "\n",
        "When we call `dolfinx.fem.assemble_vector(L)` the following happens:\n",
        " - Each process calculates the cell tensors bT for cells that it owns.\n",
        " - It assembles (adds) the result into its local array.\n",
        "\n",
        "At this point no parallel communication has taken place! The vector is in an inconsistent state, a contributiong to a degree of freedom might have taken place in another process.\n",
        "\n",
        "To update the values we use:\n",
        "```\n",
        "x.scatter_rev(ScatterMode.add)\n",
        "```"
      ],
      "metadata": {
        "id": "Hd07BpTN9i7f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile linear_form.py\n",
        "\n",
        "from mpi4py import MPI\n",
        "from petsc4py import PETSc\n",
        "import dolfinx\n",
        "import ufl\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "\n",
        "\n",
        "comm = MPI.COMM_WORLD\n",
        "mesh = dolfinx.mesh.create_unit_square(comm, 2, 2, ghost_mode=dolfinx.mesh.GhostMode.none)\n",
        "V = dolfinx.fem.FunctionSpace(mesh, (\"Lagrange\", 1))\n",
        "u = dolfinx.fem.Function(V)\n",
        "v = ufl.TestFunction(V)\n",
        "\n",
        "L = ufl.inner(5.0, v)*ufl.dx\n",
        "\n",
        "b = dolfinx.fem.assemble_vector(dolfinx.fem.form(L))\n",
        "\n",
        "if comm.rank == 0:\n",
        "    print(\"Before scattering\")\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {b.array}\")\n",
        "    comm.Barrier()\n",
        "\n",
        "b.scatter_reverse(dolfinx.la.ScatterMode.add)\n",
        "\n",
        "if comm.rank == 0:\n",
        "    print(\"\\nAfter scattering\")\n",
        "\n",
        "for i in range(comm.size):\n",
        "    if comm.rank == i:\n",
        "      print(f\"Data on process {comm.rank}: {b.array}\")\n",
        "    comm.Barrier()"
      ],
      "metadata": {
        "id": "a0bPoYG_-Wf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! mpirun -n 2 --allow-run-as-root --oversubscribe python3 linear_form.py"
      ],
      "metadata": {
        "id": "SjESK-Ti-vUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assembling matrices in parallel\n"
      ],
      "metadata": {
        "id": "0MXiKpiNAEuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solving a linear system in parallel"
      ],
      "metadata": {
        "id": "rCFkGRllAP0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parallel Scaling\n",
        "\n",
        "High performance computing has two common notions of scalability:\n",
        "\n",
        " - Strong scaling is defined as how the solution time varies with the number of processors for a fixed total problem size.\n",
        " - Weak scaling is defined as how the solution time varies with the number of processors for a fixed problem size per processor."
      ],
      "metadata": {
        "id": "qAAUV84KAVRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 8\n",
        "\n",
        "Scalability study: [poisson](https://github.com/IgorBaratta/FEniCSxCourse/blob/main/Problem8_HPC/poisson.py)\n"
      ],
      "metadata": {
        "id": "f2o86GuiBvQW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}